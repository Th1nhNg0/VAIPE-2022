{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"8QAj6pt1D5UB"},"outputs":[],"source":["from glob import glob\n","import pandas as pd\n","from PIL import Image,ImageOps,ImageFile\n","import os\n","from tqdm import tqdm\n","import re\n","import difflib\n","ImageFile.LOAD_TRUNCATED_IMAGES = True"]},{"cell_type":"code","source":["new_train_path='image_crop'\n","file_paths='public_train\\pill\\images\\*.jpg'\n","csv_path='combine_train.csv'"],"metadata":{"id":"jU56bw60D_Fs"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2Sdh_mMLD5UF"},"outputs":[],"source":["files=glob(file_paths)\n","# windows only\n","files = [f.replace('\\\\', '/') for f in files]\n","len(files)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t3XIv3DeD5UF"},"outputs":[],"source":["# if path not exit\n","if not os.path.exists(new_train_path):\n","    os.makedirs(new_train_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IhDuAHfbD5UG"},"outputs":[],"source":["df = pd.read_json('public_train\\pill_pres_map.json')\n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PRBPvg2hD5UG"},"outputs":[],"source":["def remove_accents(input_str):\n","    s1 = u'ÀÁÂÃÈÉÊÌÍÒÓÔÕÙÚÝàáâãèéêìíòóôõùúýĂăĐđĨĩŨũƠơƯưẠạẢảẤấẦầẨẩẪẫẬậẮắẰằẲẳẴẵẶặẸẹẺẻẼẽẾếỀềỂểỄễỆệỈỉỊịỌọỎỏỐốỒồỔổỖỗỘộỚớỜờỞởỠỡỢợỤụỦủỨứỪừỬửỮữỰựỲỳỴỵỶỷỸỹ'\n","    s0 = u'AAAAEEEIIOOOOUUYaaaaeeeiioooouuyAaDdIiUuOoUuAaAaAaAaAaAaAaAaAaAaAaAaEeEeEeEeEeEeEeEeIiIiOoOoOoOoOoOoOoOoOoOoOoOoUuUuUuUuUuUuUuYyYyYyYy'\n","    s = ''\n","    for c in input_str:\n","        if c in s1:\n","            s += s0[s1.index(c)]\n","        else:\n","            s += c\n","    return s\n","\n","def convert(txt:str):\n","    txt = txt.lower()\n","    txt = remove_accents(txt)\n","    txt = re.sub(r'^\\d+\\s*\\)\\s*', '', txt)\n","    txt = re.sub(r'\\s*sl:.+', '', txt)\n","    return txt\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c4XKk_bzD5UH"},"outputs":[],"source":["files=glob('public_train\\prescription\\label\\*.json')\n","pres_df = pd.DataFrame()\n","for file in tqdm(files):\n","    temp_df = pd.read_json(file)\n","    # add file name\n","    temp_df['file_name'] = file.split('\\\\')[-1]\n","    pres_df = pd.concat([pres_df,temp_df])\n","pres_df = pres_df[pres_df['label']=='drugname']\n","pres_df['mapping'] = pres_df['mapping'].astype(int)\n","pres_df['text']=pres_df['text'].str.replace(r'^\\d+\\s*\\)\\s+','',regex=True).str.lower().apply(lambda x: remove_accents(x))\n","pres_df.drop(['id'], axis=1, inplace=True)\n","pres_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9QIYWvxsD5UH"},"outputs":[],"source":["def is_similar(first, second, ratio):\n","    return difflib.SequenceMatcher(None, first, second).ratio() > ratio\n","    \n","def findid(text,pres_df):\n","    df = pd.DataFrame([text],columns =['text'])\n","    result = [s for f in df['text'] for s in pres_df['text'] if is_similar(f,s, 0.9)]\n","    df = pd.DataFrame(result,columns =['text'])['text'].unique()\n","    df = pd.DataFrame(df,columns =['text'])\n","    if not df.empty:\n","      # print(df.merge(pres_df,on = 'text' )['mapping'].values)\n","      return df.merge(pres_df,on = 'text' )['mapping'].unique().tolist() # Chỉnh khúc này nếu muốn dùng xác xuất\n","    else:\n","      return None"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7gk5ffdQD5UI"},"outputs":[],"source":["df = pd.read_json('public_train\\pill_pres_map.json')\n","result = []\n","\n","image_count = 0\n","for index, row in tqdm(df.iterrows(), total=len(df)):\n","    temp_df = pd.read_json('public_train\\prescription\\label\\\\'+row['pres'])\n","    temp_df = temp_df[temp_df['label']=='drugname']\n","    mapping = temp_df['mapping'].astype(int).values\n","    vector = [0]*108\n","    vector[107] = 1\n","    for i in mapping:\n","        vector[i] = 1\n","    temp_df['text']=temp_df['text'].str.replace(r'^\\d+\\s*\\)\\s+','',regex=True).str.lower().apply(lambda x: remove_accents(x))\n","    for text in temp_df.text:\n","        ids = findid(text,pres_df)\n","        for id in ids:\n","            vector[id] = 1\n","    for pill in row['pill']:\n","        image_name=pill.replace('json','jpg')\n","        image_path = f'public_train\\pill\\image\\{image_name}'\n","        temp_df = pd.read_json(f'public_train\\pill\\label\\{pill}')\n","        img = Image.open(image_path)\n","        img = ImageOps.exif_transpose(img)\n","        for row in temp_df.iloc:\n","            label = row['label']\n","        for row in temp_df.iloc:\n","            x1 = row['x']\n","            y1 = row['y']\n","            x2 = x1 + row['w']\n","            y2 = y1 + row['h']\n","            label = row['label']\n","            crop_img = img.crop((x1, y1, x2 , y2))\n","            save_path = f'{new_train_path}/{image_count}.jpg'\n","            crop_img.save(save_path)\n","            image_count+=1\n","            result.append((save_path, vector,label))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7FhQsVC3D5UI"},"outputs":[],"source":["final_df = pd.DataFrame(result, columns=['pill_path', 'vector','label'])\n","final_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TWnk3ySqD5UJ"},"outputs":[],"source":["final_df.to_csv(csv_path)"]}],"metadata":{"kernelspec":{"display_name":"Python 3.9.12 ('yolo5')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.5"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"19320f378bb735297497afff6af98d5c6dd8ace19a9d10dc3f83f214c51ae416"}},"colab":{"name":"image_crop.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}